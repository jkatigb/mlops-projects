# Production-Ready ML Project Template

## Overview

A comprehensive, production-ready cookiecutter template for ML projects that implements MLOps best practices:

* **Data Versioning**: DVC integration with support for S3, GCS, Azure, and local storage
* **Experiment Tracking**: MLflow and Weights & Biases integration
* **Code Quality**: Pre-commit hooks with black, isort, flake8, mypy, bandit, and more
* **Testing**: pytest with coverage, integration tests, and performance benchmarks
* **CI/CD**: Multi-stage GitHub Actions pipeline with security scanning and automated deployment
* **Containerization**: Multi-stage Docker builds with security best practices
* **Model Monitoring**: Data drift detection and performance monitoring
* **API Service**: FastAPI-based model serving with authentication and rate limiting
* **Documentation**: MkDocs with automatic API documentation

## Features

### ğŸš€ Production-Ready Components

- **Modular Architecture**: Clean separation of data, features, models, and API layers
- **Configuration Management**: Hydra-based config with environment-specific overrides
- **Structured Logging**: JSON-formatted logs with correlation IDs
- **Error Handling**: Comprehensive error handling with retry mechanisms
- **Security**: Secret scanning, vulnerability detection, and secure defaults
- **Scalability**: Support for distributed training and async predictions

### ğŸ› ï¸ MLOps Best Practices

- **Reproducible Pipelines**: DVC pipelines with automatic dependency tracking
- **Model Registry**: Centralized model versioning and metadata storage
- **A/B Testing**: Built-in support for model comparison and gradual rollouts
- **Feature Store**: Consistent feature engineering across training and serving
- **Model Validation**: Automated testing for model performance and behavior

### ğŸ“Š Monitoring & Observability

- **Metrics Collection**: Prometheus-compatible metrics export
- **Distributed Tracing**: OpenTelemetry integration
- **Alerting**: Configurable alerts for drift and performance degradation
- **Dashboards**: Pre-built Grafana dashboards for model monitoring

## Quick Start

```bash
# Install cookiecutter
pip install cookiecutter

# Create new project from template
cookiecutter gh:yourname/cookiecutter-ml-project-dvc-precommit

# Navigate to project
cd your-project-name

# Set up development environment
make setup

# Run the pipeline
make pipeline

# Start API service (if enabled)
make api-dev
```

## Project Structure

```
your-project/
â”œâ”€â”€ .github/workflows/     # CI/CD pipelines
â”œâ”€â”€ config/               # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml      # Main configuration
â”‚   â””â”€â”€ params.yaml      # Model parameters
â”œâ”€â”€ data/                # Data directory (DVC-managed)
â”‚   â”œâ”€â”€ raw/            # Original, immutable data
â”‚   â”œâ”€â”€ processed/      # Cleaned, processed data
â”‚   â””â”€â”€ features/       # Feature-engineered data
â”œâ”€â”€ docs/                # Project documentation
â”œâ”€â”€ models/              # Trained models (DVC-managed)
â”œâ”€â”€ notebooks/           # Jupyter notebooks
â”œâ”€â”€ reports/             # Generated analysis and metrics
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ api/            # API service
â”‚   â”œâ”€â”€ data/           # Data processing
â”‚   â”œâ”€â”€ features/       # Feature engineering
â”‚   â”œâ”€â”€ models/         # Model training and evaluation
â”‚   â”œâ”€â”€ monitoring/     # Model monitoring
â”‚   â””â”€â”€ utils/          # Utility functions
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ .dvc/               # DVC configuration
â”œâ”€â”€ .pre-commit-config.yaml  # Pre-commit hooks
â”œâ”€â”€ dvc.yaml            # DVC pipeline definition
â”œâ”€â”€ Dockerfile          # Container definition
â”œâ”€â”€ Makefile           # Development tasks
â”œâ”€â”€ pyproject.toml     # Project metadata and dependencies
â””â”€â”€ README.md          # Project documentation
```

## Configuration Options

The template supports extensive configuration through `cookiecutter.json`:

- **Python Version**: 3.10, 3.11, or 3.12
- **License**: MIT, Apache 2.0, BSD-3, GPL-3.0, or Proprietary
- **DVC Remote**: S3, Google Cloud Storage, Azure Blob, or local
- **Experiment Tracking**: MLflow, Weights & Biases, or both
- **Cloud Provider**: AWS, GCP, Azure, or none
- **Container Registry**: Docker Hub, ECR, GCR, ACR
- **Optional Features**:
  - GPU support
  - Data validation (Great Expectations + Pandera)
  - Model monitoring (Evidently)
  - API service (FastAPI)
  - CI/CD tool selection

## Development Workflow

### 1. Data Pipeline

```bash
# Pull latest data
make dvc-pull

# Run data validation
dvc repro validate_data

# Process data
dvc repro prepare_data

# Engineer features
dvc repro feature_engineering
```

### 2. Model Training

```bash
# Train model with current config
make train

# Run hyperparameter tuning
python -m src.models.tune --config config/tuning.yaml

# Evaluate model
make evaluate
```

### 3. Testing

```bash
# Run all tests
make test

# Run specific test types
pytest -m unit
pytest -m integration
pytest -m slow

# Check code quality
make quality
```

### 4. Deployment

```bash
# Build Docker image
make docker-build

# Push to registry
make ecr-push  # or gcr-push, acr-push

# Deploy to production
make deploy-aws  # or deploy-gcp, deploy-azure
```

## Security Considerations

- **Secret Management**: Uses environment variables and secret scanning
- **Dependency Scanning**: Automated vulnerability detection with Safety and pip-audit
- **Container Scanning**: Trivy integration for Docker image vulnerabilities
- **Access Control**: API authentication with JWT tokens
- **Data Privacy**: Built-in support for data anonymization

## Performance Optimization

- **Caching**: Multi-level caching for data, features, and predictions
- **Parallelization**: Distributed processing with Dask/Ray support
- **Model Optimization**: Quantization and pruning utilities
- **Serving Optimization**: Model warm-up and batching strategies

## Monitoring and Alerts

- **Model Drift**: Statistical tests for feature and prediction drift
- **Performance Metrics**: Real-time tracking of latency, throughput, and accuracy
- **Resource Usage**: CPU, memory, and GPU utilization monitoring
- **Business Metrics**: Custom KPI tracking and reporting

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Run `make quality` to ensure code standards
5. Submit a pull request

## License

This template is available under the {{cookiecutter.open_source_license}} license.

---
*Status*: Production-Ready ğŸš€ 
