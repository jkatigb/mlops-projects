# Example values for a FastAPI ML model serving deployment

## Image configuration
image:
  repository: myregistry/iris-classifier
  tag: "v1.2.0"
  pullPolicy: IfNotPresent

## Service configuration
service:
  type: LoadBalancer
  port: 80
  targetPort: 8000
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"

## Ingress configuration (optional, if using ingress instead of LoadBalancer)
ingress:
  enabled: false
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
  hosts:
    - host: iris-model.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: iris-model-tls
      hosts:
        - iris-model.example.com

## Resource configuration
resources:
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

## Autoscaling configuration
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
      selectPolicy: Max

## Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 1

## Security Context
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 2000
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false  # FastAPI may need to write temp files

## Health checks
livenessProbe:
  enabled: true
  httpGet:
    path: /health
    port: http
  initialDelaySeconds: 30
  periodSeconds: 10

readinessProbe:
  enabled: true
  httpGet:
    path: /ready
    port: http
  initialDelaySeconds: 10
  periodSeconds: 5

## Environment variables
env:
  - name: MODEL_NAME
    value: "iris-classifier"
  - name: LOG_LEVEL
    value: "INFO"
  - name: WORKERS
    value: "4"
  - name: MAX_BATCH_SIZE
    value: "32"

## Monitoring
monitoring:
  enabled: true
  prometheus:
    scrape: true
    port: 9090
    path: /metrics
  serviceMonitor:
    enabled: true
    interval: 30s
    labels:
      release: prometheus-operator

## Model configuration
model:
  type: sklearn
  version: "1.2.0"
  loading:
    timeout: 60
  inference:
    batchSize: 32
    timeout: 10
    maxConcurrent: 100

## Node affinity for better performance
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - model-serving
        topologyKey: kubernetes.io/hostname